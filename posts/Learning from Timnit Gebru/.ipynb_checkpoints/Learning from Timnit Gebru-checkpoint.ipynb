{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fdbfe9-e2c4-4c82-a9c8-365fee747f65",
   "metadata": {},
   "source": [
    "# Dr. Timnit Gebru comes to Middlebury\n",
    "\n",
    "On Monday, Apr 24, Dr. Timnit Gebru is coming to Middlebury to give a talk on Computer Vision and \"Artificial General Intelligence.\" Dr. Gebru is one of the leading voices on the ethics and impacts of AI. Motivated by her own experiences with race- and gender-based discrimination, she has undertaken projects looking at things like the reliability (or lack thereof) of facial recognition software to identify Black women. She is also a strong advocate for accountability for Big Tech, and in 2020 was fired from her then-job at Google after the company tried to prevent her from publishing a paper on the dangers of AI language models. Since then, she has focused on independent research to further public knowledge of the risks and biases of AI. She is a brave and outspoken voice in her field, and is inspiring because of her tumultuous upbringing – she immigrated to the US seeking asylum from war in her home country – and her breaking-down of gender and racial barriers in a famously insular industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4224ff-4947-4e42-b6a3-b36e4ef67e15",
   "metadata": {},
   "source": [
    "### Fairness, Accountability, Transparency, and Ethics in Computer Vision\n",
    "It is pretty widely accepted that issues with AI recognition softwares such as algorithms trained to identify the gender of a subject in an image, are hindered by the datasets they are trained on, and that they are trained with a bias towards white men. But Dr. Gebru points out in her talk on Fairness, Accountability, Transparency, and Ethics in Computer Vision, that datasets are a secondary issue. The real issue, she claims, is that these technologies exist in the first place. Black women are the group worst classified by gender recognition models, but race and gender are both social constructs, so as Dr. Gebru points out, gender recognition models are based on a flawed assumption. She also notes that facial recognition is not a purely objective tool since its purpose is often to identify and prosecute minority groups. Mass surveilance has made it easier than ever to discriminate against racial minorities and other marginalized groups. Her point is that the training data is only a small part of the issue with such technologies. The larger issue is why they exist in the first place, and what they are used for.\n",
    "#### tl;dr\n",
    "The data facial recognition models are trained on is only a small part of the issue with them; the greater issue is the application of these models as a way of targeting minorities and marginalized groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33917720-3c2f-4de4-99e1-ab5f9e434ac2",
   "metadata": {},
   "source": [
    "### Question(s) for Dr. Gebru\n",
    "1. Are there ways in which facial recognition software can be made inherently less biased? Or do its *applications* need to change for it to become a fairer technology?\n",
    "    - Tied into this, do you see any reasonable application for such software, or is it flawed at its core?\n",
    "2. What are some positive trends you see, if any, in AI/ML's objectivity and fairness towards marginalized groups?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451]",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
