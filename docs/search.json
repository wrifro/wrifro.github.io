[
  {
    "objectID": "posts/Blog_3/Penguins Blog.html",
    "href": "posts/Blog_3/Penguins Blog.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "I want to answer this question right off the bat. I think there are two answers: First, penguins are cool and cute. But more importantly, the process that we are going to use in this blog post is one that we can replicate for different datasets in the future. It doesn’t really matter that the data we will classify is penguins - it could be for the titanic, or the iris dataset. What matters is understanding how to use different machine learning techniques to develop an accurate model for classifying our data. Maybe the model we use in this process isn’t ideal for another dataset, or maybe we will need more than just 1 qualitative feature for a different dataset. However, the steps in this blog post are generally adaptable to a variety of situations.\nWith that out of the way, let’s dive into it: first up is prepping our data so that it is ready for our model.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nI selected a Decision Tree Classifier for this process because I wanted to experiment with a new model. Using cross validation, I tried every possible different combination of 1 qualitative + 2 quantitative features, updating my list of top columns based on whichever yielded the highest mean score from the crossval subsets.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)']\n\ntop_score = 0\ntop_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    cvs = cross_val_score(clf,X_train[cols],y_train,cv = 5)\n    \n    if cvs.mean() > top_score:\n        top_cols = cols\n        top_score = cvs.mean()\n\nprint(top_cols,top_score)\n\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.9803921568627452\n\n\n\n\n\nIt looks like islands are the best qualitative feature for fitting a classifier. Why is this?\nWe can filter our training data to see how many different islands the 3 species call home:\n\ntrain.groupby(\"Species\")[\"Island\"].nunique()\n\nSpecies\nAdelie Penguin (Pygoscelis adeliae)          3\nChinstrap penguin (Pygoscelis antarctica)    1\nGentoo penguin (Pygoscelis papua)            1\nName: Island, dtype: int64\n\n\nInteresting! So the Adelie Penguin is the only species on all three islands – Chinstrap and Gentoo are only on one each.\n\ntrain[[\"Island\",\"Species\"]].groupby(\"Island\").value_counts()\n\nIsland     Species                                  \nBiscoe     Gentoo penguin (Pygoscelis papua)            101\n           Adelie Penguin (Pygoscelis adeliae)           35\nDream      Chinstrap penguin (Pygoscelis antarctica)     56\n           Adelie Penguin (Pygoscelis adeliae)           41\nTorgersen  Adelie Penguin (Pygoscelis adeliae)           42\ndtype: int64\n\n\nThe code above lets us break down the numbers by island. Torgerson has only Adelie Penguins, Dream has Chinstraps and Adelies, and Biscoe has Gentoos and Adelies. Based on this, it makes sense that island is the best qualitative feature to use in the model. Knowing which island we are considering instantly simplifes the selection process since we are only choosing between at most two species.\n\n\n\nCulmen length and depth were the two features selected. Let’s see how these vary by species.\n\ntrain[[\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby(\"Species\").mean()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Species\n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      38.710256\n      18.365812\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      48.719643\n      18.442857\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      47.757000\n      15.035000\n    \n  \n\n\n\n\n\ntrain[[\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby(\"Species\").mean().plot.bar(rot=20)\n\n<AxesSubplot: xlabel='Species'>\n\n\n\n\n\nFrom here, we can clearly see that Adelie Penguins have the shortest culmens, while Chinstraps’ culmens are slightly longer than Gentoos’. Gentoos have the smallest culmen depth, while Adelie and Chinstrap are fairly similar in culmen depth. When you consider that, once you know the island, you really only need to be able to distinguish between TWO species (Adelie + one of either Chinstrap or Gentoo), not between all three, it becomes clear that the differences in Culmen Length alone are probably sufficient to make a fairly accurate prediction. Culmen depth is another helpful feature to make the classifier even more accurate.\n\n\nIs a difference in environment enough to change the physical characteristics of a species? Or in this case, of one species, since Adelie penguins are the only ones to appear on more than one island?\n\ntrain[[\"Island\",\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby([\"Island\",\"Species\"]).mean()\n\n\n\n\n\n  \n    \n      \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      Species\n      \n      \n    \n  \n  \n    \n      Biscoe\n      Adelie Penguin (Pygoscelis adeliae)\n      38.674286\n      18.228571\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      47.757000\n      15.035000\n    \n    \n      Dream\n      Adelie Penguin (Pygoscelis adeliae)\n      38.395122\n      18.319512\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      48.719643\n      18.442857\n    \n    \n      Torgersen\n      Adelie Penguin (Pygoscelis adeliae)\n      39.056098\n      18.529268\n    \n  \n\n\n\n\nIt doesn’t look like it, although Adelie Penguins’ Culmen Lengths and Depths are both a tiny bit greater on Torgerson Island than the other two. Maybe the lack of competition from other penguins is responsible for the evolution of a special population of GIANT Adelie penguins on Torgerson. My takeaway is that in 1000 years, Torgerson Island’s penguins will have ENORMOUS culmens.\nBut in all seriousness, no real difference here.\n\n\n\n\nHere, we read in the test data, then score our classifier on the test data filtered to contain only our selected features. We DO NOT fit the classifier to the testing data, because we want it to be scored based on the model trained on the training data. So we first fit it to the TRAINING data, then score it on the testing data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nclf.fit(X_train[top_cols],y_train)\n\nX_test, y_test = prepare_data(test)\nclf.score(X_test[top_cols],y_test)\n\n0.9852941176470589\n\n\n98.5% accuracy. Pretty close to perfect!\n\n\n\nI adapted this technique from the scikit learn documentation page: https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html\nThis shows us the depth of the tree, and how many samples remain to classify at each step. This tree has a depth of 4, which means it needs to make a maximum of 4 decisions to reach a classification for a sample.\n\nfrom sklearn.tree import plot_tree\nfrom matplotlib import pyplot as plt\n\nplot_tree(clf, filled=True)\nplt.title(\"Decision tree trained on 3 Penguin Features: Culmen Length, Depth, and Island\")\nplt.show()\n\n\n\n\n\n\n\n\nX_plot = X_test[top_cols]\nqual_features = [\"Island_Biscoe\",\"Island_Dream\",\"Island_Torgersen\"]\nx0 = X_plot[X_plot.columns[3]]\nx1 = X_plot[X_plot.columns[4]]\n\n\nfrom matplotlib.patches import Patch\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 5))\n\ngrid_x = np.linspace(x0.min() - 1,x0.max() + 1,501)\ngrid_y = np.linspace(x1.min() - 1,x1.max() + 1,501)\nxx, yy = np.meshgrid(grid_x, grid_y)\n\nXX = xx.ravel()\nYY = yy.ravel()\n\nfor i in range(len(qual_features)):\n    XY = pd.DataFrame({\n      X_plot.columns[3] : XX,\n      X_plot.columns[4] : YY\n    })\n\n    for j in qual_features[::-1]:\n        XY.insert(0,j,0)\n\n    XY[qual_features[i]] = 1\n    \n    p = clf.predict(XY.values)\n    p = p.reshape(xx.shape)\n\n    # use contour plot to visualize the predictions\n    axarr[i].contourf(xx, yy, p, cmap='jet',alpha = 0.2,vmin = 0, vmax = 2)\n\n\n    ix = X_plot[qual_features[i]] == 1\n    # plot the data\n    axarr[i].scatter(x0[ix], x1[ix], c = y_test[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n    axarr[i].set_title(label = qual_features[i])\n    axarr[i].set(xlabel = X_plot.columns[3], \n        ylabel  = X_plot.columns[4])\n\n    patches = []\n    for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n\n        plt.tight_layout()\n\n\n\n\nThis is an interesting vizualization. It gives us a glimpse at the blocky nature of the decision tree classifer - rather than separating these data with a single straight line, a Decision Tree Classifier almost breaks the plot up into rectangles.\nThis is because the Decision Tree Classifer works by dividing along one of the two axes at each iteration of the fit process. So it can only divide vertically or horizontally. Over time, it makes smaller and more precise rectangular subsections that classify the data more and more accurately.\n\n\n\nA simple test is to run our classifier with a lower depth and see how well it performs.\n\nclf2 = DecisionTreeClassifier(max_depth = 4)\nclf3 = DecisionTreeClassifier(max_depth = 3)\n\nclf2.fit(X_train[top_cols], y_train)\nscore4 = clf2.score(X_test[top_cols], y_test)\n\nclf3.fit(X_train[top_cols], y_train)\nscore3 = clf3.score(X_test[top_cols], y_test)\n\nscore4,score3\n\n(0.9852941176470589, 0.9852941176470589)\n\n\n\nscore4 == score3\n\nTrue\n\n\nIt looks like the classifier can run well at lower depths. The model still performs well at a depth of 4, and 3. However, even with the depth of five that the classifier automatically selected, the accuracy stays the same and there is no real gap between testing and training data. This means that the model is not overfit to the training data, so while the greater depth may not add anything, it doesn’t detract from the model either.\nIn summary, this was an interesting process. Starting off knowing nothing about the distribution of the species or their physical characteristics, we were able to automate most of the process of building a model to classify them. This speaks to the power of machine learning and how quickly it can break down a complicated process to reach the desired end result.\nObviously not every dataset can be 100% accurately classified as this penguin one could, but a process like this gets us pretty close.\nAnd finally, to honor the three heroes of this blog post, here they are in all their glory:\n  Adelie Penguin  img source\n Chinstrap Penguin img source\n Gentoo Penguin  img source"
  },
  {
    "objectID": "posts/Blog_4/Linear Regression.html",
    "href": "posts/Blog_4/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Source code: https://github.com/wrifro/wrifro.github.io/blob/main/posts/Blog_4/LinearRegression.py"
  },
  {
    "objectID": "posts/Blog_4/Linear Regression.html#conclusion",
    "href": "posts/Blog_4/Linear Regression.html#conclusion",
    "title": "Linear Regression",
    "section": "Conclusion",
    "text": "Conclusion\nThe power of the linear regression model is apparent from these tests. Rather than focusing on labeling datapoints, it is able to come up with a model that predicts what a hypothetical datapoint’s value should be based on the values of the other datapoints. There are slight differences between the different approaches to the model - stochastic gradient descent achieves an optimal result with impressive speed, for example - but at the end of the day all of the different approaches yield more or less the same result."
  },
  {
    "objectID": "posts/Blog_5/Unsupervised Learning with Linear Algebra.html",
    "href": "posts/Blog_5/Unsupervised Learning with Linear Algebra.html",
    "title": "Image Compression with Singular Value Decomposition",
    "section": "",
    "text": "Compressing files is a key computational procedure. One approach to image compression is Singular Value Decomposition, where a matrix is broken down into its nonzero (singular) values and two orthagonal matrices. By choosing only a subset of the singular values from which to reconstruct the image, we can achieve significant reductions in image size while retaining a high degree of clarity. In this blog, I explore the process of breaking down and reconstructing an image using different subsets of an image’s singular values.\n\nLet’s begin by reading in an image and converting it to grayscale. I chose the famous Earthrise image from the Apollo 8 mission.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = 'https://www.nasa.gov/sites/default/files/thumbnails/image/apollo08_earthrise.jpg'\nimg = read_image(url)\n\n\n\ncode\nfig, axarr = plt.subplots(1, 2, figsize = (10, 5))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\ngrey_img.shape\n\n(2190, 2280)\n\n\n\n\nWe can calculate the SVD of the gray image using the formula \\(A = UDV^T\\) where U and V are orthoganal matrices and D has nonzero entries only on its diagonal. The np.linalg.svd function gives us the values needed to reconstruct D.\n\nU, sigma, V = np.linalg.svd(grey_img)\nD = np.zeros_like(grey_img,dtype=float) # matrix of zeros of same shape as A\nD[:min(grey_img.shape),:min(grey_img.shape)] = np.diag(sigma)        # singular values on the main diagonal\n\n(2190, 2280)\n\n\nApplying this process to a smaller subset of U, D, and V - that is to say, the first k columns of U, the top k values in D, and the first k rows of V - lets us approximate the original image.\nBelow, the function svd_reconstruct does this for an image and a value k. It first calculates the singular value decomposition of an image, then reconstructs it and returns based on the number of singular values specified by the users.\n\n\nshow it to me\ndef svd_reconstruct(img,k,compFactor):\n    if(compFactor > 0): # if you would rather specify degree of compression than enter a value for k, set compFactor to\n                        # a value greater than zero. For example, calling svd_reconstruct(img = img,k = 0, compFactor = 10)\n                        # would compress the image to 10% of its original size.\n                \n        origSize = img.shape[0] * img.shape[1]\n        desiredSize = (origSize/compFactor)\n        k = round(desiredSize/(img.shape[0] + img.shape[1] + 1))\n        \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img,dtype=float) # matrix of zeros of same shape as A\n    D[:min(img.shape),:min(img.shape)] = np.diag(sigma) # singular values on the main diagonal\n    \n    U_ = U[:,:k] # take a m * k subset of U\n    D_ = D[:k, :k] # and the top k values of D\n    V_ = V[:k, :] # and an n * k subset of V\n    A_ = U_ @ D_ @ V_ # and multiply them all together to reconstruct the image!\n    \n    return(A_)\n\nreconImg = svd_reconstruct(grey_img,30,0) # Image reconstructed with top 30 singular values\nplt.imshow(reconImg,cmap = 'Greys')\nplt.axis('off')\n\n\n(-0.5, 2279.5, 2189.5, -0.5)\n\n\n\n\n\n\n\n\nLogically, an uncompressed image’s size is proportional to the number of pixels. So, \\(m \\cdot n\\). for an \\(m\\) x \\(n\\) image\nFor compressed images, the size is directly proportional to k. We can calculate it using the formula \\(k(m + n + 1)\\). We get this because we multiply k*m values in U by a k*k matrix in D, although it actually only has a size of k since values are only along the diagonal, and a k*n subset of V. This gives \\(k*m + k * k*n = k(m + 1 + n)\\).\nBelow, we define a function to calculate the compression ratio: \\(\\frac{originalSize}{compressedSize}\\) and the percent storage \\(\\frac{compressedSize}{originalSize} \\cdot 100\\)\n\n\nclick here to\ndef compression(img,k):\n    m = img.shape[0]\n    n = img.shape[1]\n    origSize = m * n\n    compressedSize = k * (m + n + 1)\n    percentStorage = compressedSize/origSize * 100\n    compressionRatio = origSize/compressedSize\n    \n    return percentStorage,compressionRatio\n\n\n\n\n\nIn our svd_reconstruct function, we can specify a degree of compression instead of the number of singular values to use, and use the math shown above to calculate the corresponding k values and generate a reconstruction.\n\nreconImg2 = svd_reconstruct(grey_img,0,20)\nreconImg3 = svd_reconstruct(grey_img,0,4)\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\naxarr[0].imshow(reconImg2,cmap = 'Greys')\naxarr[0].set(title = 'reconstructed image, storage = 5%')\naxarr[0].axis('off')\naxarr[1].imshow(reconImg3,cmap = 'Greys')\naxarr[1].set(title = 'reconstructed image, storage = 25%')\naxarr[1].axis('off')\n\n(-0.5, 2279.5, 2189.5, -0.5)\n\n\n\n\n\n\n\n\n\n\nBelow, we show the original image next to an image reconstructed using 30 singular values.\nClearly both show the same thing, but the reconstruction is much less vibrant and is more pixelated.\n\nimport math as math\ndef compare_images(A, A_,k):\n    plt.rcParams.update({'font.size': 8})\n\n    fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n\n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = f\"reconstructed image, percent storage = {round(compression(grey_img,k)[0],2)}%\")\n\ncompare_images(grey_img, reconImg,30)\n\n\n\n\nLet’s vizualise the increase in clarity as we use more singular values. The function experiment makes 8 reconstructions using a range of singular values 5-40. By 40 components, we are getting quite close to an accurate representation of the original, though we aren’t quite there yet.\n\ndef experiment(img):\n    plt.rcParams.update({'font.size': 12})\n    imgArr = []\n    for i in range(8):\n        reconstructed = svd_reconstruct(img, (i + 1) * 5)\n        imgArr.append(reconstructed)\n        \n    fig, axarr = plt.subplots(2, 4, figsize = (25,15))\n    i = 0\n    for ax in axarr.ravel():\n        ax.imshow(imgArr[i],cmap = 'Greys')\n        ax.axis(\"off\")\n        ax.set(title = f\"{(i+1) * 5} components, \\nstorage = {round(compression(grey_img,(i + 1) * 5)[0],2)}%\")\n        i += 1\n\n\nexperiment(grey_img)\n\n\n\n\nLet’s run a similar experiment, but this time increasing the number of singular values exponentially, starting with 10, to see how the reconstruction looks with a larger number of singular values.\nWe can see that by the time we hit 100 components, we are really close, and by the time we hit 1000, we are basically at the original, although file size isn’t much smaller than the original by that point.\n\ndef experiment2(img):\n    plt.rcParams.update({'font.size': 12})\n    imgArr = []\n    for i in range(3):\n        reconstructed = svd_reconstruct(img, 10 ** (i+1),0)\n        imgArr.append(reconstructed)\n        \n    fig, axarr = plt.subplots(1, 3, figsize = (25,15))\n    i = 0\n    for ax in axarr.ravel():\n        ax.imshow(imgArr[i],cmap = 'Greys')\n        ax.axis(\"off\")\n        ax.set(title = f\"{10 ** (i+1)} components, \\nstorage = {round(compression(grey_img,10 ** (i + 1) )[0],2)}%\")\n        i += 1\n    plt.tight_layout()\n    \nexperiment2(grey_img)\n\n\n\n\nI think taken on their own it’s hard to see the difference between the original and the reconstructions as they get better and better. The reconstruction from 40 singular values, for instance, appears quite clear. But the biggest difference is the intensity of the color as well as some detail on the moon’s surface. It’s really clear when you set reconstructions side by side with the original. Below is a reconstruction with 60 singular values where the difference in clarity to the original image is readily apparent.\n\nrecon60 = svd_reconstruct(grey_img,60,0)\n\ncompare_images(grey_img, recon60,60)\n\n\n\n\nStill, while this reconstruction doesn’t perfectly match the original, it is an excellent approximation and uses only about 5% of the storage space taken by the original image. This is a testament to the power of the SVD process to reduce the size of images.\n\n\n\nThe experiments above show the power of SVD image reconstruction. For a given image, the svd_reconstruct function can accept either k, the number of singular values to use, or compFactor, the compression factor we want to achieve, and generate a reconstruction of the original image. Clearly, there are some exciting applications of this process, and it is capable of reproducing a fairly accurate representation of the original image while maintaining a high degree of clarity."
  },
  {
    "objectID": "posts/Blog_2/GradientDescentBlog.html",
    "href": "posts/Blog_2/GradientDescentBlog.html",
    "title": "Gradient Descent Blog",
    "section": "",
    "text": "Logistic Regression Source Code: click here"
  },
  {
    "objectID": "posts/Blog_2/GradientDescentBlog.html#conclusion",
    "href": "posts/Blog_2/GradientDescentBlog.html#conclusion",
    "title": "Gradient Descent Blog",
    "section": "Conclusion",
    "text": "Conclusion\nBoth the regular gradient descent and stochastic gradient descent are exciting and powerful. They do what the perceptron algorithm cannot, and find the best line if two sets of points are not linearly separable. They run on multiple dimensions, and are quite efficient even if not perfect. Given more time I would have loved to implement stochastic gradient with momentum, but didn’t quite have the time to make it even more efficient. Something for the future."
  },
  {
    "objectID": "posts/example-blog-post/firstPost.html",
    "href": "posts/example-blog-post/firstPost.html",
    "title": "Intro blog post",
    "section": "",
    "text": "And now I’ll try adding it in markdown:\ndef addTwoNumbers(num_a, num_b):\n    sum = num_a + num_b\n    return sum\naddTwoNumbers(3,5)\nLet’s see if this post works…"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference.\n---\ntitle: Intro blog post\nauthor: Wright Frost\ndate: '2023-02-16'\ndescription: 'Just playing around with markdown and quarto'\nformat: html\n---\n\n\n\nI’m going to add some python code:\ndef addTwoNumbers(num_a, num_b):\n    sum = num_a + num_b\n    return sum\naddTwoNumbers(3,5)\n```\n\n\n\nWill this work and make a new post? Not sure. Hopefully yes.\nAnyway….. my name is Wright, I’m a Computer Science and Geography Double major, and this is my attempt at a blog post!"
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html",
    "href": "posts/Blog_1/PerceptronBlog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Perceptron source code: click here"
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html#how-does-the-perceptron-algorithm-work",
    "href": "posts/Blog_1/PerceptronBlog.html#how-does-the-perceptron-algorithm-work",
    "title": "Perceptron Blog",
    "section": "How does the perceptron algorithm work?",
    "text": "How does the perceptron algorithm work?\nThe key part of the perceptron algorithm is the update step. In my code, this is done in both the fit and update methods.\nMathematically, this can be represented as: (on a personal note this is the first thing I have ever done in LaTeX and I’m really proud of it)\n\\(\\hat{w}^{(t+1)} = \\hat{w}^t + \\mathbb{1}(\\tilde{y}_i(\\vec{\\hat{w}^t} \\cdot \\vec{\\tilde{x}_1}) <0){\\tilde{y}_i} {\\tilde{x}_1}\\)\nWithin the fit() method, this is done in the following code:\ny_hat = np.dot(w,X_i)\nw_i = ( (np.multiply(y_hat, y_i) >= 0) * w_i)  + ( (np.multiply(y_hat, y_i) < 0) * (w_i + np.multiply(y_i,X_i)) )\nThe logic is actually fairly simple. Y_hat represents the predicted score for X_i, the current index/value being considered. If y_hat is correctly classified, multiplying it by y_i should yield True, equal to 1 in Python. The other part of the equation checks to see if this product is less than 1 (using 1s and -1s instead of 1s and 0s for our y values makes this possible). If it is, this expression will evaluate to True, which is equal to 1 in Python.\nOnly one of these expressions is true at any given time. When False, these evaluate to zero.\nIf the first expression is True, aka when it is equivalent to 1, it preserves the current weight vector, while the second expression, equal to zero, nullifies its coefficient.\nWhen the second expression is True and therefore equal to 1, it is multiplied by the sum of w_i (the current weight vector) and the product of the feature matrix (X) at index i, and the classification matrix (y) at index i. This sounds complicated, but really this line of code is just either keeping the old weight vector if it classified point i correctly, or adding or subtracting the value it incorrectly classified to ensure that it classifies it correctly in the next iteration.\nThis logic is the same in the update method. The only difference is that update accepts a weight vector as a parameter, running only a single step of the Perceptron algorithm rather than to completion."
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html#if-the-data-are-linearly-separable-then-the-perceptron-algorithm-will-converge.",
    "href": "posts/Blog_1/PerceptronBlog.html#if-the-data-are-linearly-separable-then-the-perceptron-algorithm-will-converge.",
    "title": "Perceptron Blog",
    "section": "If the data are linearly separable, then the perceptron algorithm will converge.",
    "text": "If the data are linearly separable, then the perceptron algorithm will converge.\nLet’s vizualise the process of improvement by showing the progression of the Perceptron algorithm over time.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\np2 = Perceptron()\n\n\n\n\n\nLet’s see how the algorithm updates over time…\nThese three plots only show instances where the weight vector updates (that is to say, when the algorithm misclassifies a point and updates the weight vector to reflect this). This ensures that we actually see how the Perceptron algorithm corrects itself over time. Otherwise, we might just see three cases where it correctly classifies a point and nothing changes. BORING!!!\n\nw_next = np.random.rand(p_features)\n\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\nX_1 = np.append(X, np.ones((X.shape[0], 1)), 1)\nscore_prev = 0\n\nfor ax in axarr.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    w_prev = w_next\n    ax.scatter(X[:,0],X[:,1], c = y)\n    draw_line(w_prev, -10, 10,ax,linestyle = \"dashed\") \n    done = False\n    while done == False:\n        w_next, score, i = p2.update(X,y,w_prev)\n        if (score_prev != score) or score == 1 :\n            done = True\n    score_prev = score\n    draw_line(w_next, -10, 10,ax)\n    ax.scatter(X[i, 0], X[i, 1],color = \"none\", facecolors = \"none\", edgecolors = \"red\")\n    accuracy = (score)\n    ax.set_title(f\"accuracy = {accuracy}\")\n    \nplt.tight_layout()\n\n\n\n\nIt is fairly easy to see that the algorithm updates when it missclassifies a point, gradually improving until it achieves convergence.\n\n\nWhat if the data are not linearly separable?\nLet’s repeat the same experiment as above, only this time, using two clouds of points that are NOT linearly separable.\nWe can achieve this easily with the make_blobs function by making the centers closer to one another.\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.3), (1.7,0.3)])\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nplt.rcParams[\"figure.figsize\"] = (8, 6)\nfig1, axarr1 = plt.subplots(2, 3, sharex = True, sharey = True)\n\nw_next1 = np.random.rand(p_features)\n\nscore_1 = 0\n\nfor ax in axarr1.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    w_prev1 = w_next1\n    ax.scatter(X2[:,0],X2[:,1], c = y2)\n    draw_line(w_prev1, -10, 10,ax,linestyle = \"dashed\") \n    done = False\n    while done == False:\n        score_prev = score1\n        w_next1,score1,i = p2.update(X2,y2,w_prev1)\n        if (score_prev != score1) or score1 == 1 :\n            done = True\n    draw_line(w_next1, -10, 10,ax)\n    ax.scatter(X2[i, 0], X2[i, 1],color = \"none\", facecolors = \"none\", edgecolors = \"red\")\n    accuracy = (score1)\n    ax.set_title(f\"accuracy = {accuracy}\")\n    \nplt.tight_layout()\n\n\n\n\nWe can see that the algorithm still updates and tries to correct itself, but is unable to achieve 100% accuracy. Including more subplots helps to show that this process of guessing but never achieving convergence will continue infinitely.\n\n\nCan Perceptron work in more than 2 dimensions?\nOnly one way to find out…\nLet’s start by generating a 5D feature matrix.\n\nX5d = np.random.rand(100,5)\ny5d = np.random.choice([0, 1], size=100)\n\nWe now have a random 5d feature matrix, and a random binary classification for each point.\nLet’s try to run it with 10,000 max steps…\n\np3 = Perceptron()\n\np3.fit(X5d,y5d, max_steps = 10000)\n\nDid it work?\n\np3.score(X5d,2*y5d-1)\n\n0.6\n\n\nIt works! It makes sense that it wouldn’t achieve convergence on a completely random set of points and classifications.\nIt is worth asking, though, is it theoretically possible to have a linearly separable set of points in 5 (or more) dimensions?\nIt is pretty straightforward to see that this is possible in 3 dimensions - imagine a flat plane separating 2 clouds of points in 3 dimensions.\nIn 5 dimensions, all that this means is that the weight vector multiplied by the feature matrix correctly classifies each point – either one or zero. One does not need to imagine what a 5D physical space might look like to see that this is certainly achievable.\n\n\nPerceptron Runtime\nFinally, let’s consider the question of how long it takes for the update step to run.\nWhat is actually happening? Just multiplication and some logical comparisons, really – we check to see if mathematically the product of our weight vector and the feature matrix evaluates to what we would expect it to be, and if not, we perform some simple matrix multiplication and addition to get our new weight vector. This is dependent on the number of features (p), but not on the number of points (n). Therefore, this operation takes O(p) time. It does not matter how many points there are, because we are only focusing on a single data point at a time, and updating only relative to that point. So the only variable impacting the time complexity of this operation is p – the number of features."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html",
    "href": "posts/Final Project/Final Blog Post.html",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "",
    "text": "Source code: https://github.com/ebwieman/wildfire-risk-tool/blob/main/main.ipynb\nMapping tool: https://github.com/ebwieman/wildfire-risk-tool/blob/main/wildfire_risk_mapping_tool.ipynb"
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#abstract",
    "href": "posts/Final Project/Final Blog Post.html#abstract",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Abstract",
    "text": "Abstract\nWildfires have increased in recent years as a result of climate change, posing a threat to humans and the environment. Understanding the conditions that lead to wildfire and which areas are most at risk is important for developing mitigation strategies and allocating resources. In this project, we utilize machine learning algorithms to predict both wildfire occurrence and area of wildfires. To predict wildfire occurrence, we trained several machine learning models on a dataset containing meteorological information about potential wildfires in Algeria. An accuracy of 100% was achieved using a logistic regression model trained on all features. To predict wildfire area, models were trained on a dataset of wildfire events from Montesinho National Park, Portugal. The area prediction task was much more difficult than the classification task and required additional label transformation to achieve higher accuracies. The highest area prediction accuracy was achieved with a logistic regression model trained on all features and using labels transformed with the scikit-learn lab encoder. Lastly, a model trained on the Algerian dataset was used to predict and map wildfire risk in the United States. The trained model was more simplistic than other models due to the lack of US meteorological data available, but visual comparison to existing fire prediction tools such as the USGS Fire Danger Map Tool shows that the model was somewhat able to predict fire risk in the United States. One shortcoming of this work is that all datasets used to train our models were small and regionally specific, making it difficult to create generalizable tools for use on larger scales or different regions. Developing larger and more regionally dispersed wildfire datasets will aid in future creation of more robust fire prediction tools."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#introduction",
    "href": "posts/Final Project/Final Blog Post.html#introduction",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Introduction",
    "text": "Introduction\nClimate change has increased the likelihood of a variety of extreme weather events, including wildfires. These extreme events pose definite risks to both human and ecological communities. At the same time, machine learning is emerging as an important predictive tool in natural disaster prevention; numerous studies have already used machine learning to classify risk of natural hazards. For example, Youssef et al. used machine learning models to predict an area’s susceptibility to landslides, floods, and erosion in Saudi Arabia (Youssef et al. 2023). This study experimented with a few different models, ultimately settling on Random Forest. In Choubin et al.’s study of avalanches in mountainous regions, Support Vector Machine (SVM) and Multivariate Discriminant Analysis were found to be the best models in assessing avalanche risk based on meteorological and locational data (Choubin et al. 2019).\nPrior studies have also focused specifically on wildfire prediction. A 2023 paper even developed a new model to better predict burned areas in Africa and South America (Li et al. 2023). In addition, the dataset used in our project to predict Portuguese fires was originally the topic of a 2007 paper focusing on training models on readily available meteorological data to predict the area of wildfires (Cortez and Morais 2007). This study also experimented with different predictive features and models, finding SVM and random forest to be the best predictors (Cortez and Morais 2007).\nIn this project, we build on these prior studies, using many similar techniques and models. We use two datasets to predict wildfire likelihood and wildfire area given meteorological data. Our first dataset, the Algerian Forest Fires Dataset, contains fire and non-fire events with 11 attributes that describe weather characteristics at the time of that event. Our second dataset, the Forest Fires Dataset, contains data from Montesinho National Park, Portugal. This dataset contains many of the same weather characteristics as the Algerian dataset, but contains only fire events, and also includes the areas of these events. Rather than using this dataset to predict whether a fire occurred in a given area, we used it to predict the size a fire is likely to reach given certain meteorological conditions.\nOver the course of this project, we followed in the footsteps of prior studies, experimenting with different models to predict risk. We implemented several existing machine learning models such as Random Forest and Logistic Regression, ultimately choosing the models and sets of features that yielded the highest accuracy score. We trained and validated models on the Algerian dataset to predict whether or not a forest fire will occur given certain meteorological conditions and also trained/validated models on the Portugal dataset to predict forest fire area given many of the same features.\nAlthough we trained our models with localized data, we also assessed the extent to which we could apply our models across datasets and geographies. Similarly to the multi-hazard susceptibility map produced by Youssef et al., we created a fire susceptibility map of fire risk using our models and county-level temperature and precipitation data for the entire United States (Youssef et al. 2023). While we cannot assess the accuracy of this map directly, we can compare it to existing US wildfire prediction tools to at least visually assess our accuracy.\nFinally, we build on existing research to assess the ethics of using machine learning models in predicting risk of natural hazards. As Wagenaar et al. caution in their 2020 study of how machine learning will change flood risk and impact assessment, “applicability, bias and ethics must be considered carefully to avoid misuse” of machine learning algorithms (Wagenaar et al. 2020).\nAs extreme weather events become more prevalent with climate change, we must learn how to best predict and manage these events. The methods explored detail our own attempt to do so."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#values-statement",
    "href": "posts/Final Project/Final Blog Post.html#values-statement",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Values Statement",
    "text": "Values Statement\nBeing able to accurately predict the risks of wildfires offers numerous benefits for various stakeholders. Predictive models like ours have the potential to revolutionize wildfire management and mitigation strategies, leading to better preparedness, timely response, and ultimately, the protection of lives and property. Thus, we wanted to build upon prior studies and further the ability to accurately predict wildfires.\nThe main beneficiaries of this project are the wildfire departments and emergency responders. These agencies can gain valuable insights into the probability and severity of wildfire occurrences in specific areas though our project. These predictions allow for better allocation of resources and more effective mitigation strategies, reducing the overall damage and saving tons of valuable resources. If we can assess areas with high-risk for wildfires ahead of time, they could be better equipped and protected.\nInsurance companies can also leverage our predictive models to assess risks associated with wildfires. By integrating these algorithms into their underwriting processes, insurers can accurately evaluate the potential impact of wildfires on properties, allowing for more precise risk assessment. Moreover, the models can aid in post–fire analysis, enabling insurance companies to provide timely assistance to policyholders living in at-risk areas.\nGovernment agencies responsible for land and forest management are also another one of our potential users. Policymakers can make more informed decisions regarding land use, forest management, and allocation of resources for fire prevention efforts. Ultimately, this can lead to more proactive measures such as improved firebreak construction and targeted vegetation management.\nAnother group that may not directly utilize our predictive wildfire models, but still stands to benefit is the general population. Information that trickles down from fire management authorities can help individuals living in fire-prone areas make informed decisions and take the necessary precautions. People are able to take property protection measures and implement evacuation plans they deem necessary. Overall, the predictions from these models can increase safety measures while minimizing property damage within the predicted at-risk areas.\nWhile the use of machine learning models to predict wildfire risks can be highly beneficial, there are potential downsides with their implementation that should be considered. Our models rely heavily on well documented weather and geographical data. Data collection takes time and money, which could potentially be a barrier to using our models. Remote areas without government or research funding most likely will not be able to produce the data needed to benefit from our models. Moreover, communities lacking internet access, computing devices, or technological literacy are unable to take advantage of our models. This can disproportionately affect rural areas and low-income communities, further exacerbating existing inequalities.\nAdditionally, we recognize that our models are trained and tested on data that is in English. This language barrier can hinder individuals who don’t have the proficiency in the language, limiting their ability to use these predictive models. Additionally, cultural differences and contextual nuances might not be well captured in our models, leading to potential misunderstandings and biases. Thus, we want to be mindful of the potential barriers and hope to address these shortcomings in our future work. Failing to address these disparities could perpetuate social inequalities in wildfire management.\nUpon reflecting these potential harms and benefits, we still believe this project will improve wildfire management and be a crucial resource to communities in fire-prone areas. Additionally, this project furthers our understanding of factors contributing to wildfires. With this information, we can accurately predict the likelihood of wildfires in a given region, enabling better fire management, mitigation, and evacuation. Wildfire predictions can help protect the land, wildlife, and local communities. Still, efforts should be made to actively involve marginalized groups in wildfire preparedness and response initiatives."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#materials-and-methods",
    "href": "posts/Final Project/Final Blog Post.html#materials-and-methods",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Materials and Methods",
    "text": "Materials and Methods\n\nThe Datasets\nIn this project, we utilized various machine learning models from scikit-learn to better understand wildfire occurrences. Specifically, we trained and tested on two datasets to predict wildfire likelihood and wildfire area given the meteorological and geographical features. These datasets were taken from the University of California Irvine Machine Learning Repository. Our first dataset, the Algerian Forest Fires Dataset, contains fire and non-fire occurrences with 11 attributes describing weather characteristics at the time the event occurred. Our second dataset, the Forest Fires Data, has data from Montesinho National Park, Portugal. This dataset contains many of the same weather attributes as the Algerian one. However, rather than using this dataset to predict whether a fire occurred in a given area, it can be used to predict the size of the fire given certain meteorological and geographical conditions.\nThe Algerian dataset has 224 instances that regrouped data from two regions in Algeria, namely the Bejaia region located in the northeast of Algeria and the Sidi Bel-abbes region in the northwest of Algeria. There are 122 instances for each of the two regions. This data was collected from June 2012 to September 2012. As stated, the dataset contains fire and non-fire events with 13 attributes that describe weather conditions. The 13 features are: day, month, year, temperature in Celsius, relative humidity, wind speed, rain, fine fuel moisture code, duff moisture code, drought code, initial speed index, build up index, and fire weather index. Our target label is “fire”, with a label of 1.0 meaning that a fire has occurred and 0.0 being no fire.\nThe data collected in the Portuguese dataset was taken from Montesino park in the northeast region of Portugal. The 12 features in this dataset are similar to the ones in the Algerian dataset. These features include: x-axis spatial coordinate, y-axis spatial coordinate, month, day, fine fuel moisture code, duff moisture code, drought code, initial speed index, temperature in Celsius, relative humidity, wind, and rain. The target label is “fire area”, which determines the total burned area of a fire given the meteorological and geographical conditions.\nWe used RidgeCV from scikit-learn to pull out five important features in each dataset that we later trained on. RidgeCV is a cross validation method in ridge regression. The higher absolute coefficient, the more important that feature is. After applying RidgeCV to the Portuguese dataset, we yielded the five most important features: x-axis spatial coordinate, y-axis spatial coordinate, initial speed index, temperature, and wind. The five most important features for the Algerian dataset are: relative humidity, rain, fine fuel moisture code, initial speed index, and fire weather index.\n\n\nModeling\nBoth datasets were trained on various algorithms taken from scikit-learn and each model was analyzed based on their accuracy score.\nThe Algerian training data was trained on four different algorithms: decision tree, logistic regression, random forest, and stochastic gradient descent. For each of the algorithms, we trained the Algerian training set with its complete set of 13 features and also with the 5 features selected via our feature selection process. Additionally, we used cross validation to evaluate the models’ performance at different settings and to avoid overfitting. All five algorithms performed relatively well on the training data, with the accuracy score ranging from 0.552 to 0.990. Training the complete set of features on the stochastic gradient descent classifier yielded the lowest score of 0.552. Alternatively, training on the logistic regression classifier with the complete set of features gave us the highest accuracy of 0.990. Therefore, we went ahead and used the logistic regression model to test our Algerian data on.\nAs for the Portuguese dataset, we started by training it on the linear regression model and support vector machine for regression, but got very low accuracy scores of 0.048 and -0.026, respectively. It was much harder to train on this dataset because the labels were non-binary and many were skewed towards 0’s. We then performed a log-transform on our labels, hoping to improve the accuracy. However, this did not make much of a difference. The highest score we got after log-transforming our labels and training on a linear regression model is -0.020. After this, we went ahead and transformed our labels through three different methods. First, we used the lab encoder in the scikit-learn preprocessing package to encode the target labels with values between 0 and n number of classes - 1. Second, we divided the labels into 50 different ranges. Last, we transformed the labels into binary labels, making the areas larger than 0.0 as “non 0’s” and keeping labels of 0.0 as “0.0”. While we recognize that this method is not ideal, it was very difficult training the Portuguese dataset since it is a very small dataset and most of the target labels are skewed towards 0.0.\nIn the end, we trained our dataset on four different models: linear regression, logistic regression, support vector machine for classification, and support vector machine for regression. Each model was trained on different combinations of features and transformed target labels. We trained the model on its complete set of features, on the features selected via our feature selection process, and on the features highlighted in the research paper written by (Cortez and Morais 2007). In their paper, they also experienced difficulty training this Portuguese dataset. Ultimately, transforming our labels via the lab encoder got us a training score of 0.513 when trained on a logistic regression model using the complete set of features. Unsurprisingly, changing our y into binary labels and into ranges got us a perfect training accuracy of 1.0. Thus, we decided to test our Portuguese data on the logistic regression model with the transformed target labels.\n\n\nUS Meteorological Data and Mapping Tool\nCounty level precipitation and temperature data was downloaded from the NOAA NClimGrid dataset (NoaaData?). Specific months of interest were selected and data was downloaded for that month as two csvs, one containing precipitation data and one containing temperature data. The precipitation data contained total precipitation in mm for each day of the month for each US county, while the temperature data contained maximum temperature in Celsius for each day. Significant time was spent searching for additional meteorological data, specifically wind speed and humidity, but these efforts were ultimately unsuccessful. The temperature and precipitation datasets were uploaded to GitHub and then loaded into the Mapping Tool Jupyter notebook. Substantial data cleaning was performed to prepare the US data for prediction. Data cleaning tasks included renaming columns after the datasets were read in and resolving discrepancies in the state and county codes so that data frames could be merged by county. The precipitation and temperature datasets were read in, converted to pandas dataframes, and merged. From this dataframe, data was extracted for a specified day and county-level wildfire predictions were generated for that day. Predictions were generated using a new Random Forest model that was trained on the Algeria dataset using only Temperature and Rain as features, as these were the only features we had access to for US data. These predictions were then joined with a dataframe containing geological coordinates for each county and then the predictions were mapped. The final algorithm used for mapping allows the user to put in a date and state and then produces a map of county-level fire risk for that state on the specified day. If there is no data for the specified day, the algorithm returns a message apologizing for the lack of data. If no state is specified, the algorithm returns a map of the entire continental US. While the maps could not be rigorously assessed for accuracy, visual comparison to the USGS Fire Danger Map Tool was used to discuss the accuracy of our Algerian model when applied to US data (UsgsData?)."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#results",
    "href": "posts/Final Project/Final Blog Post.html#results",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Results",
    "text": "Results\nWe came close to achieving all of our goals for this project. Our greatest success was our modeling of the Algerian dataset. We achieved high training accuracy with several models, including > 95% training accuracy for a Random Forest Classifier, Logistic Regression, and Decision Tree Classifier, as well as 100% testing accuracy with a Logistic Regression model. It’s difficult to say how applicable this model is outside of the region of Algeria the data are from. The random forest classifier that we trained on the Algerian model for our mapping tool got many things right – it predicts more widespread fire in the summer months, for example, than in the winter – but we trained it on many fewer features than are in the full Algerian dataset.\nThis is certainly one of the shortcomings of our project. The lack of weather data available for the United States means that we can’t fully say how well our model would perform on data from a region other than Algeria. However, there are positive trends, and were more weather observations available, it seems likely that our model would have some applicability.\nDespite the fact that we did not get to realize the full abilities of our model, creating an interactive map is nonetheless a highlight of the project. It shows that our project has the ability to be broadly applicable, and underscores the power of Machine Learning as a tool for natural hazard prediction and prevention. We explored the ethical dimensions of this application of Machine Learning in a short essay, and while there are certainly ethical considerations that must be made when training models to predict natural hazards, this project shows that there is also a great deal of opportunity to use Machine Learning to predict and manage risks from natural hazards.\nWhile we cannot assess the accuracy of our US prediction tool directly, visual comparison to existing mapping tools such as the USGS Fire Danger Map Tool yields some insights. If we compare the two tools’ predictions for March 2, 2023, we see that our tool vastly overpredicts fire risk in the US (Figure 1). Our tool correctly predicts fire in the locations that the USGS identifies the most high risk, specifically Southwestern Texas and Arizona, but we also predict risk of fire in large swaths of the Midwest and Florida that the USGS tool does not deem particularly high risk.\n \n\nFig 1: Comparison of fire risk predictions by our mapping tool (top) and the USGS Fire Danger Map Tool (bottom) for March 2, 2023.\n\nNext we look at a potentially more interesting case in July 2020, the peak of summer, when we would expect more areas of the country to be susceptible to wildfires. Here the USGS predicts risk of fire in much of the Southwestern United States, and we see that our model does as well. However, we once again see overprediction by our model, specifically in the Midwest. One possible contributing factor is that the USGS model differentiates between agricultural land and does not predict fire risk in these areas. Our model seems to predict fire risk in many of the areas deemed agricultural by the USGS model, so differentiating between land cover classes could be useful for future development of this tool. Additionally, our model predicts on the county scale, while the USGS mapping tool appears to have much better resolution, allowing it to make more refined predictions. One other exciting observation in the July maps is that while our model tends to make generally uniform predictions across states, we do see some agreement between the two tools in identifying pockets of low fire risk within the Southwestern United States. The fact that our mapping tool tends to make relatively uniform predictions across states suggests that the model could just be learning general weather patterns rather than actually learning fire risk. Training on additional features would likely help address this problem, but this is impossible at the moment due to the lack of aggregated US weather data available.\n \n\nFig 2: Comparison of fire risk predictions by our mapping tool (top) and the USGS Fire Danger Map Tool (bottom) for July 2, 2020.\n\nThe final area of our project to discuss is the models we trained for the Portuguese dataset. Replicating the process of the original academic paper didn’t yield results of the same accuracy as those obtained by the study (Cortez and Morais 2007). Log-transforming the data was a complex process that was perhaps more involved than we had anticipated. However, we still managed to achieve about 50% accuracy with both Logistic Regression and Support Vector Machine models, and 100% accuracy when we transformed to binary labels and categorical labels. Figure 3, shown below, shows the results the model achieved on 20 features from the test set:\n\n\n\nimage\n\n\n\nFig 3: While the model correctly predicts 35% of fire areas, it gets close on a number of them, so 35% is not reflective of the actual performance. We see that the model does the best at predicting small fires, though for medium-sized fires, it usually either far overshoots or far undershoots in its prediction.\n\nFigure 4 shows the same observations, predicted using the SVC model. It also gets 35% of its predictions correct, but unlike the LR model, it only ever underestimates the area of fires, while the LR model overestimates several. This suggests that the SVC model is slightly more conservative, and may fail to predict the largest fires.\n\n\n\nimage\n\n\n\nFig 4: The SVC model seems to be more conservative, predicting the area of all fires – even those that are larger in size – to be 0 ha or close to it.\n\nThese mixed results for the Portuguese data, along with our high testing accuracy for the Algeria dataset, shows that predicting whether or not a fire occurred is a much more straightforward task than predicting the area of the fire. There is certainly room to grow in our modeling of this dataset; perhaps trying different models or features would yield different results. We recognize that transforming the target labels into categorical and binary labels isn’t ideal; however, it was too difficult otherwise. This was because the Portuguese dataset is a very small dataset with the majority of the target labels skewed towards 0.0. Thus, this problem was turned into a classifying problem instead of producing an accurate predictor model. Nonetheless, our process and results show that it is possible to build a model trained on readily-available weather observations that predicts the area of fires with a reasonable degree of accuracy."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#concluding-discussion",
    "href": "posts/Final Project/Final Blog Post.html#concluding-discussion",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOverall, our project was successful in a number of ways, and opens the door for future research and experimentation. Our analysis of wildfire risk provided us with an opportunity to practice data science. We successfully found regional data sets, cleaned them, and prepared them for machine learning analyses. Using a Ridge model, we successfully selected features to use for making predictions in each dataset, and through experimenting with a variety of different machine learning models, we selected two that reached reasonably high accuracy. Through examining United States county-level data, we were able to apply our models across scales. Finally, we visualized our results well, and effectively discussed their ethical implications.\nAs a group, we met and even surpassed our original project goals. We have a code repository that details our data preparation and cleaning, model training and prediction, and evaluation, notebooks with in exploratory data analysis and testing of our models, a short essay about the implications of using machine learning and automated tools for ecological forecasting (both risks and benefits) and a map of wildfire risk in the United States constructed using our models and US meteorological data. We had wanted to use machine learning to help with natural hazard risk detection and prevention, and our final product provides detailed and thorough documentation of our attempt to do so.\nBecause our project built on the work of scholars such as Cortez and Morias, we have the ability to compare our results to past work. While Cortez and Morias’ 2007 study that explored the Portugal data we used in our project identified SVM to be the best machine learning model for fire area prediction, we found SVC and Logistic Regression to be equally effective on the Portugal data set (Cortez and Morais 2007). Our work weaves in well with prior studies, allowing us to make predictions about natural risk by choosing and applying machine learning models. Overall, through experimenting with different models, we ultimately were able to predict wildfires and wildfire area more accurately than our base rates, and experimented with applying these models across scales. With more time, data, or computational resources, we could improve our methods and findings. Spending more time finding and downloading more data for the United States (such as wind speed and humidity, prehaps) would likely improve our map and our model and allow us to better apply the model across space. While we trained our machine learning models on regionally-specific data, we applied thier predictions to new and differently-scaled geographies. When applying models across space, it is important to do so in collaboration those people who may be affected by the results of predictive machine learning tools (Wagenaar et al. 2020). Overall, when we pay attention to bias and applicabiltiy, wildfire and natural disaster models have numerous applications, and even the potential to save lives. As the likelihood and intensity of extreme weather events continues increasing in the face of climate change, we should incorporate and continue building on the anaysis presented here to acheive even better methodologies to predict and prepare for these events."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#group-contributions-statement",
    "href": "posts/Final Project/Final Blog Post.html#group-contributions-statement",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nMadeleine: Near the beginning of the project, Wright and I worked on preparing the data from the Algeria and Portugal data sets to eventually split into testing and training data. I then worked on selecting features important for fire/fire area prediction using RidgeCV. We then worked on visualizing the relationships between these selected features and fire likelihood/area. Later in the project, I worked with Eliza on our map. Specifically, I worked on rendering the map to show all the counties in the US, and and helped prepare to join the counties on the map to the county-level temperature and precipitation data. I also worked on the map’s design. Finally, I tried to find additional weather data for the US (such as wind speed), but was largely unsuccessful.\nWright: I worked with Madeleine at the beginning of the project doing some data preparation. Specifically, I worked on formatting the Algeria data set so that it could be treated as binary fire/no fire data. I then worked on data vizualizations, making graphs of the relationships between different features and fire occurrence/scale. I wasn’t directly involved with the modeling, but after the models were trained I ran them on some test observations and vizualized their performance. Finally, I worked on a short research essay about the ethics of using Machine Learning for natural hazard prediction, prevention, and adaptation.\nEliza: I helped with initial downloading and importing of the Portugal and Algeria datasets and splitting them into train and test sets. I initially worked on training models on the Portugal dataset (with little success) and did some research to figure out how the authors of the paper that the dataset originated from achieved higher accuracies. I did some of the initial experimentation with log-transforming our y values, but then I pivoted to working on the mapping tool and Nhi took this part of the project over. I definitely spent most of my time and effort working on the mapping tool for this project with Madeleine. I did extensive research on US county-level meteorological data and identified and downloaded the nClimGrid dataset that we ended up using for our US predictions. I did a lot of the data cleaning of these datasets prior to generating our maps, which Madeleine took the lead on, and I helped streamline the map-making process after creating our first map by converting a lot of our code into reusable functions. For the blog post, I took lead on the Materials and Methods section with Nhi, specifically focusing on the mapping tool methods, and the Abstract.\nNhi: Towards the beginning of the project, I helped with cleaning up our datasets and making sure they are ready to be trained on. For the majority of the project, I did a lot of the training and testing for both of the Portuguese and Algerian datasets. The Algerian dataset was a lot easier to train on, and I was able to obtain a very good accuracy score early on. I trained the Algeria dataset on different models and with different combinations of features to see which would yield the highest accuracy. I did a lot of cross validation and fine tuning to make sure the models weren’t overfitting. Then I tested the highest scoring model on the Algerian testing dataset. Eliza started with training the Portuguese dataset, but did not get very far since this was a much harder dataset to train on. Thus, I took over the modeling for this as well. I started by training the dataset on other models since Eliza had only trained on two models to start. I wasn’t very successful with the other models either, so I explored different ways to transform the target labels since the majority of the labels were skewed towards 0’s. I did a lot of digging around on scikit-learn and while not ideal, I transformed the labels via lab encoder, into binary labels, and into different ranges of the target labels. I then trained different combinations of the features and transformed target labels on different models. I analyzed these models to see how they performed against the testing dataset. Overall, I dissected a lot of different variations of the features and target labels. Additionally, I also combed through our code and cleaned them up. I also added a lot of the descriptors before and after each code section to explain our processes. For the blog post, I wrote the Values Statement, the dataset and modeling in the Materials and Methods section, and added a bit to the Results."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#ethics-of-ml-as-a-tool-for-natural-hazard-prediction-prevention",
    "href": "posts/Final Project/Final Blog Post.html#ethics-of-ml-as-a-tool-for-natural-hazard-prediction-prevention",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Ethics of ML as a tool for Natural Hazard Prediction & Prevention",
    "text": "Ethics of ML as a tool for Natural Hazard Prediction & Prevention\nMachine Learning is emerging as an important predictive tool in natural disaster prevention. Numerous studies have used machine learning to classify risk of natural hazards, from landslides, floods, and erosion in Saudi Arabia (Youssef et al. 2023), to avalanche risk in mountainous regions (Choubin et al. 2019), to area of forest fires in the tropics (Li et al. 2023). In fact, the Portuguese fire data used in this project was originally the topic of a 2007 paper focusing on training models on readily available real-time meteorological data to predict the area of wildfires (Cortez and Morais 2007).\nSo, there is a growing library of academic literature and projects using Machine Learning models to predict either the occurrence or the scale of natural hazards. Our project builds on these works, using many similar techniques and models. As our high testing accuracy for the Algerian dataset shows, machine learning clearly can be a powerful tool in natural hazard prediction.\nBut this numerical, scientific, approach comes with risks. As Cortez and Morais note in their study of Monteshino National Park in Portugal, most fires are caused by humans, which is why features like day of the week are important. Natural hazards only become natural disasters when they have a negative impact on people’s lives. As Youseff et al. observe in their study of multi-hazard prediction in Saudi Arabia, multiple hazards often occur at the same time, and the impact of natural hazards disproportionately affects impoverished and underdeveloped countries (Youssef et al. 2023). A risk of our approach is that our models focus only on the fires, disregarding both human influence on the landscape that may lead to increased risk of hazards (e.g. overdrawing from local water sources, deforestation, etc.), as well as the impact fires may have on humans. Predicting whether or not a fire will occur, or how large it will be, is only useful if it is applied to help those at risk from fires.\nWagenaar et al.’s 2020 paper “Invited perspectives: How machine learning will change flood risk and impact assessment” touches on some of these ethical considerations (Wagenaar et al. 2020). One risk they bring up is that improved knowledge of flood risk from ML models might result in protection of only high-value land or property owned by the wealthy. That example is certainly transferable to our fire hazard project. They also raise the question of privacy, noting that some people might not want it widely known that their home is considered “at-risk” for flooding, or other hazards. Finally, there is the question of data. The causes of hazards in one place may not cause the same hazard in another, so it is important to understand which human and geographic features influence hazard risk at a local scale rather than trying to train a one-size-fits-all model.\nWith all that in mind, our project needs to come with an asterisk. We have trained models for forest fire occurrence and scale in two specific places. It is therefore unreasonable to expect that our model will perfectly transfer to other places in the world. If our project were to be used for any application beyond the academic, we would need to make sure that its impact – whether that be for risk management, insurance policies, or some other application – be equitable and nondiscriminatory. We are just scratching the surface of using Machine Learning for natural hazard prevention, and while our results show it to be a powerful tool, we must also stay vigilant to make sure that it is a force for good."
  },
  {
    "objectID": "posts/Final Project/Final Blog Post.html#personal-reflection",
    "href": "posts/Final Project/Final Blog Post.html#personal-reflection",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nI found this project to be a really rewarding culminating project of the semester. I was excited to apply what I’d learned about ML to a real-world application, and the forest-fires datasets that we found aligned nicely with my other major, Geography, because of the spatial applications. So much of Geography as an academic and professional field is about applying technical insights spatially, so this project was a good opportunity to practice this.\nI learned a lot about both forest fires and hazard prediction in general from this process. There is not a one-size fits all approach. That is probably the biggest takeaway I had. Different models work well depending on what data is available, and different regions of the world require a different approach. But I also saw the power of ML as a tool for hazard prediction. Getting 100% accuracy on the Algerian data was a huge accomplishment, and left me convinced that ML will play an important role in mitigating natural disasters in the 21st century.\nI think that I personally met all of my goals for this project. I wanted to have fun, experiment, and explore a new dimension of machine learning, and I think that I succeeded in all three. A highlight for me was writing a short essay about the ethics of Machine Learning as a tool for natural hazard prediction and prevention. I thought that the ethical side of ML was one area I hadn’t explored as much up to this point in the semester, so it was a thought-provoking exercise to write about the ethics of this particular application of ML.\nI feel that this project will have a lasting impact on my approach to natural resource management as I go forward. There’s a good chance that a future job I have will be focused on sustainability, environmental conservation, etc. Having these tools at my disposal to predict what risks are posed to natural resources will be an important part of my skillset. Even if I’m not the one building models to predict hazards, I have no doubt that ML will be at the forefront of resource and hazard managament in the 21st century. ## References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wright’s CS0451 Blog",
    "section": "",
    "text": "A write up of my final project, which uses machine learning models trained on meteorological data from Algeria and Portugal and applies them to assess wildfire risk in the US.\n\n\n\n\n\n\nMay 18, 2023\n\n\nWright Frost (in collaboration with Eliza Wieman, Madeleine Gallop, and Nhi Dang)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingular Value Decomposition and its applications to image compression\n\n\n\n\n\n\nApr 30, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing and testing a linear regression model\n\n\n\n\n\n\nApr 9, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining a model to classify penguins\n\n\n\n\n\n\nMar 25, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing and Exploring the Perceptron Algorithm\n\n\n\n\n\n\nMar 10, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing and Exploring the Perceptron Algorithm\n\n\n\n\n\n\nFeb 28, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust playing around with markdown and quarto\n\n\n\n\n\n\nFeb 16, 2023\n\n\nWright Frost\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]