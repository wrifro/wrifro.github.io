[
  {
    "objectID": "posts/Blog_2/GradientDescentBlog.html",
    "href": "posts/Blog_2/GradientDescentBlog.html",
    "title": "Gradient Descent Blog",
    "section": "",
    "text": "Logistic Regression Source Code: click here"
  },
  {
    "objectID": "posts/Blog_2/GradientDescentBlog.html#conclusion",
    "href": "posts/Blog_2/GradientDescentBlog.html#conclusion",
    "title": "Gradient Descent Blog",
    "section": "Conclusion",
    "text": "Conclusion\nBoth the regular gradient descent and stochastic gradient descent are exciting and powerful. They do what the perceptron algorithm cannot, and find the best line if two sets of points are not linearly separable. They run on multiple dimensions, and are quite efficient even if not perfect. Given more time I would have loved to implement stochastic gradient with momentum, but didn’t quite have the time to make it even more efficient. Something for the future."
  },
  {
    "objectID": "posts/example-blog-post/firstPost.html",
    "href": "posts/example-blog-post/firstPost.html",
    "title": "Intro blog post",
    "section": "",
    "text": "And now I’ll try adding it in markdown:\ndef addTwoNumbers(num_a, num_b):\n    sum = num_a + num_b\n    return sum\naddTwoNumbers(3,5)\nLet’s see if this post works…"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference.\n---\ntitle: Intro blog post\nauthor: Wright Frost\ndate: '2023-02-16'\ndescription: 'Just playing around with markdown and quarto'\nformat: html\n---\n\n\n\nI’m going to add some python code:\ndef addTwoNumbers(num_a, num_b):\n    sum = num_a + num_b\n    return sum\naddTwoNumbers(3,5)\n```\n\n\n\nWill this work and make a new post? Not sure. Hopefully yes.\nAnyway….. my name is Wright, I’m a Computer Science and Geography Double major, and this is my attempt at a blog post!"
  },
  {
    "objectID": "posts/Blog_1/Untitled1.html",
    "href": "posts/Blog_1/Untitled1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "def gradient(w1,w2):\n    (w2 * np.cos(w1 * w2),w1 * np.cos(w1 * w2))"
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html",
    "href": "posts/Blog_1/PerceptronBlog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Perceptron source code: click here"
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html#how-does-the-perceptron-algorithm-work",
    "href": "posts/Blog_1/PerceptronBlog.html#how-does-the-perceptron-algorithm-work",
    "title": "Perceptron Blog",
    "section": "How does the perceptron algorithm work?",
    "text": "How does the perceptron algorithm work?\nThe key part of the perceptron algorithm is the update step. In my code, this is done in both the fit and update methods.\nMathematically, this can be represented as: (on a personal note this is the first thing I have ever done in LaTeX and I’m really proud of it)\n\\(\\hat{w}^{(t+1)} = \\hat{w}^t + \\mathbb{1}(\\tilde{y}_i(\\vec{\\hat{w}^t} \\cdot \\vec{\\tilde{x}_1}) <0){\\tilde{y}_i} {\\tilde{x}_1}\\)\nWithin the fit() method, this is done in the following code:\ny_hat = np.dot(w,X_i)\nw_i = ( (np.multiply(y_hat, y_i) >= 0) * w_i)  + ( (np.multiply(y_hat, y_i) < 0) * (w_i + np.multiply(y_i,X_i)) )\nThe logic is actually fairly simple. Y_hat represents the predicted score for X_i, the current index/value being considered. If y_hat is correctly classified, multiplying it by y_i should yield True, equal to 1 in Python. The other part of the equation checks to see if this product is less than 1 (using 1s and -1s instead of 1s and 0s for our y values makes this possible). If it is, this expression will evaluate to True, which is equal to 1 in Python.\nOnly one of these expressions is true at any given time. When False, these evaluate to zero.\nIf the first expression is True, aka when it is equivalent to 1, it preserves the current weight vector, while the second expression, equal to zero, nullifies its coefficient.\nWhen the second expression is True and therefore equal to 1, it is multiplied by the sum of w_i (the current weight vector) and the product of the feature matrix (X) at index i, and the classification matrix (y) at index i. This sounds complicated, but really this line of code is just either keeping the old weight vector if it classified point i correctly, or adding or subtracting the value it incorrectly classified to ensure that it classifies it correctly in the next iteration.\nThis logic is the same in the update method. The only difference is that update accepts a weight vector as a parameter, running only a single step of the Perceptron algorithm rather than to completion."
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html#if-the-data-are-linearly-separable-then-the-perceptron-algorithm-will-converge.",
    "href": "posts/Blog_1/PerceptronBlog.html#if-the-data-are-linearly-separable-then-the-perceptron-algorithm-will-converge.",
    "title": "Perceptron Blog",
    "section": "If the data are linearly separable, then the perceptron algorithm will converge.",
    "text": "If the data are linearly separable, then the perceptron algorithm will converge.\nLet’s vizualise the process of improvement by showing the progression of the Perceptron algorithm over time.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\np2 = Perceptron()\n\n\n\n\n\nLet’s see how the algorithm updates over time…\nThese three plots only show instances where the weight vector updates (that is to say, when the algorithm misclassifies a point and updates the weight vector to reflect this). This ensures that we actually see how the Perceptron algorithm corrects itself over time. Otherwise, we might just see three cases where it correctly classifies a point and nothing changes. BORING!!!\n\nw_next = np.random.rand(p_features)\n\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\nX_1 = np.append(X, np.ones((X.shape[0], 1)), 1)\nscore_prev = 0\n\nfor ax in axarr.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    w_prev = w_next\n    ax.scatter(X[:,0],X[:,1], c = y)\n    draw_line(w_prev, -10, 10,ax,linestyle = \"dashed\") \n    done = False\n    while done == False:\n        w_next, score, i = p2.update(X,y,w_prev)\n        if (score_prev != score) or score == 1 :\n            done = True\n    score_prev = score\n    draw_line(w_next, -10, 10,ax)\n    ax.scatter(X[i, 0], X[i, 1],color = \"none\", facecolors = \"none\", edgecolors = \"red\")\n    accuracy = (score)\n    ax.set_title(f\"accuracy = {accuracy}\")\n    \nplt.tight_layout()\n\n\n\n\nIt is fairly easy to see that the algorithm updates when it missclassifies a point, gradually improving until it achieves convergence.\n\n\nWhat if the data are not linearly separable?\nLet’s repeat the same experiment as above, only this time, using two clouds of points that are NOT linearly separable.\nWe can achieve this easily with the make_blobs function by making the centers closer to one another.\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.3), (1.7,0.3)])\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nplt.rcParams[\"figure.figsize\"] = (8, 6)\nfig1, axarr1 = plt.subplots(2, 3, sharex = True, sharey = True)\n\nw_next1 = np.random.rand(p_features)\n\nscore_1 = 0\n\nfor ax in axarr1.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    w_prev1 = w_next1\n    ax.scatter(X2[:,0],X2[:,1], c = y2)\n    draw_line(w_prev1, -10, 10,ax,linestyle = \"dashed\") \n    done = False\n    while done == False:\n        score_prev = score1\n        w_next1,score1,i = p2.update(X2,y2,w_prev1)\n        if (score_prev != score1) or score1 == 1 :\n            done = True\n    draw_line(w_next1, -10, 10,ax)\n    ax.scatter(X2[i, 0], X2[i, 1],color = \"none\", facecolors = \"none\", edgecolors = \"red\")\n    accuracy = (score1)\n    ax.set_title(f\"accuracy = {accuracy}\")\n    \nplt.tight_layout()\n\n\n\n\nWe can see that the algorithm still updates and tries to correct itself, but is unable to achieve 100% accuracy. Including more subplots helps to show that this process of guessing but never achieving convergence will continue infinitely.\n\n\nCan Perceptron work in more than 2 dimensions?\nOnly one way to find out…\nLet’s start by generating a 5D feature matrix.\n\nX5d = np.random.rand(100,5)\ny5d = np.random.choice([0, 1], size=100)\n\nWe now have a random 5d feature matrix, and a random binary classification for each point.\nLet’s try to run it with 10,000 max steps…\n\np3 = Perceptron()\n\np3.fit(X5d,y5d, max_steps = 10000)\n\nDid it work?\n\np3.score(X5d,2*y5d-1)\n\n0.6\n\n\nIt works! It makes sense that it wouldn’t achieve convergence on a completely random set of points and classifications.\nIt is worth asking, though, is it theoretically possible to have a linearly separable set of points in 5 (or more) dimensions?\nIt is pretty straightforward to see that this is possible in 3 dimensions - imagine a flat plane separating 2 clouds of points in 3 dimensions.\nIn 5 dimensions, all that this means is that the weight vector multiplied by the feature matrix correctly classifies each point – either one or zero. One does not need to imagine what a 5D physical space might look like to see that this is certainly achievable.\n\n\nPerceptron Runtime\nFinally, let’s consider the question of how long it takes for the update step to run.\nWhat is actually happening? Just multiplication and some logical comparisons, really – we check to see if mathematically the product of our weight vector and the feature matrix evaluates to what we would expect it to be, and if not, we perform some simple matrix multiplication and addition to get our new weight vector. This is dependent on the number of features (p), but not on the number of points (n). Therefore, this operation takes O(p) time. It does not matter how many points there are, because we are only focusing on a single data point at a time, and updating only relative to that point. So the only variable impacting the time complexity of this operation is p – the number of features."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wright’s CS0451 Blog",
    "section": "",
    "text": "Implementing and Exploring the Perceptron Algorithm\n\n\n\n\n\n\nMar 10, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing and Exploring the Perceptron Algorithm\n\n\n\n\n\n\nFeb 28, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust playing around with markdown and quarto\n\n\n\n\n\n\nFeb 16, 2023\n\n\nWright Frost\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]