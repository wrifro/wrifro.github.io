[
  {
    "objectID": "posts/Learning from Timnit Gebru/Untitled.html",
    "href": "posts/Learning from Timnit Gebru/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "On Monday, Apr 24, Dr. Timnit Gebru is coming to Middlebury to give a talk on Computer Vision and “Artificial General Intelligence.” Dr. Gebru is one of the leading voices on the ethics and impacts of AI. Motivated by her own experiences with race- and gender-based discrimination, she has undertaken projects looking at things like the reliability (or lack thereof) of facial recognition software to identify Black women. She is also a strong advocate for accountability for Big Tech, and in 2020 was fired from her then-job at Google after the company tried to prevent her from publishing a paper on the dangers of AI language models. Since then, she has focused on independent research to further public knowledge of the risks and biases of AI. She is a brave and outspoken voice in her field, and is inspiring because of her tumultuous upbringing – she immigrated to the US seeking asylum from war in her home country – and her breaking-down of gender and racial barriers in a famously insular industry.\n\n\nIt is pretty widely accepted that issues with AI recognition softwares such as algorithms trained to identify the gender of a subject in an image, are hindered by the datasets they are trained on, and that they are trained with a bias towards white men. But Dr. Gebru points out in her talk on Fairness, Accountability, Transparency, and Ethics in Computer Vision, that datasets are a secondary issue. The real issue, she claims, is that these technologies exist in the first place. Black women are the group worst classified by gender recognition models, but race and gender are both social constructs, so as Dr. Gebru points out, gender recognition models are based on a flawed assumption. She also notes that facial recognition is not a purely objective tool since its purpose is often to identify and prosecute minority groups. Mass surveilance has made it easier than ever to discriminate against racial minorities and other marginalized groups. Her point is that the training data is only a small part of the issue with such technologies. The larger issue is why they exist in the first place, and what they are used for. #### tl;dr The data facial recognition models are trained on is only a small part of the issue with them; the greater issue is the application of these models as a way of targeting minorities and marginalized groups.\n\n\n\n\nAre there ways in which facial recognition software can be made inherently less biased? Or do the applications need to change for it to become a fairer technology? 1a. Tied into this, do you see any reasonable application for such software, or is it flawed at its core?\nWhat are some positive trends you see, if any,"
  },
  {
    "objectID": "posts/Learning from Timnit Gebru/Learning from Timnit Gebru.html",
    "href": "posts/Learning from Timnit Gebru/Learning from Timnit Gebru.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "On Monday, Apr 24, Dr. Timnit Gebru is coming to Middlebury to give a talk on Computer Vision and “Artificial General Intelligence.” Dr. Gebru is one of the leading voices on the ethics and impacts of AI. Motivated by her own experiences with race- and gender-based discrimination, she has undertaken projects looking at things like the reliability (or lack thereof) of facial recognition software to identify Black women. She is also a strong advocate for accountability for Big Tech, and in 2020 was fired from her then-job at Google after the company tried to prevent her from publishing a paper on the dangers of AI language models. Since then, she has focused on independent research to further public knowledge of the risks and biases of AI. She is a brave and outspoken voice in her field, and is inspiring because of her tumultuous upbringing – she immigrated to the US seeking asylum from war in her home country – and her breaking-down of gender and racial barriers in a famously insular industry.\n\n\nIt is pretty widely accepted that issues with AI recognition softwares such as algorithms trained to identify the gender of a subject in an image, are hindered by the datasets they are trained on, and that they are trained with a bias towards white men. But Dr. Gebru points out in her talk on Fairness, Accountability, Transparency, and Ethics in Computer Vision, that datasets are a secondary issue. The real issue, she claims, is that these technologies exist in the first place. Black women are the group worst classified by gender recognition models, but race and gender are both social constructs, so as Dr. Gebru points out, gender recognition models are based on a flawed assumption. She also notes that facial recognition is not a purely objective tool since its purpose is often to identify and prosecute minority groups. Mass surveilance has made it easier than ever to discriminate against racial minorities and other marginalized groups. Her point is that the training data is only a small part of the issue with such technologies. The larger issue is why they exist in the first place, and what they are used for. #### tl;dr The data facial recognition models are trained on is only a small part of the issue with them; the greater issue is the application of these models as a way of targeting minorities and marginalized groups.\n\n\n\n\nAre there ways in which facial recognition software can be made inherently less biased? Or do its applications need to change for it to become a fairer technology?\n\nTied into this, do you see any reasonable application for such software, or is it flawed at its core?\n\nWhat are some positive trends you see, if any, in AI/ML’s objectivity and fairness towards marginalized groups?"
  },
  {
    "objectID": "posts/Blog_3/Penguins Blog.html",
    "href": "posts/Blog_3/Penguins Blog.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "I want to answer this question right off the bat. I think there are two answers: First, penguins are cool and cute. But more importantly, the process that we are going to use in this blog post is one that we can replicate for different datasets in the future. It doesn’t really matter that the data we will classify is penguins - it could be for the titanic, or the iris dataset. What matters is understanding how to use different machine learning techniques to develop an accurate model for classifying our data. Maybe the model we use in this process isn’t ideal for another dataset, or maybe we will need more than just 1 qualitative feature for a different dataset. However, the steps in this blog post are generally adaptable to a variety of situations.\nWith that out of the way, let’s dive into it: first up is prepping our data so that it is ready for our model.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nI selected a Decision Tree Classifier for this process because I wanted to experiment with a new model. Using cross validation, I tried every possible different combination of 1 qualitative + 2 quantitative features, updating my list of top columns based on whichever yielded the highest mean score from the crossval subsets.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)']\n\ntop_score = 0\ntop_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    cvs = cross_val_score(clf,X_train[cols],y_train,cv = 5)\n    \n    if cvs.mean() > top_score:\n        top_cols = cols\n        top_score = cvs.mean()\n\nprint(top_cols,top_score)\n\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.9803921568627452\n\n\n\n\n\nIt looks like islands are the best qualitative feature for fitting a classifier. Why is this?\nWe can filter our training data to see how many different islands the 3 species call home:\n\ntrain.groupby(\"Species\")[\"Island\"].nunique()\n\nSpecies\nAdelie Penguin (Pygoscelis adeliae)          3\nChinstrap penguin (Pygoscelis antarctica)    1\nGentoo penguin (Pygoscelis papua)            1\nName: Island, dtype: int64\n\n\nInteresting! So the Adelie Penguin is the only species on all three islands – Chinstrap and Gentoo are only on one each.\n\ntrain[[\"Island\",\"Species\"]].groupby(\"Island\").value_counts()\n\nIsland     Species                                  \nBiscoe     Gentoo penguin (Pygoscelis papua)            101\n           Adelie Penguin (Pygoscelis adeliae)           35\nDream      Chinstrap penguin (Pygoscelis antarctica)     56\n           Adelie Penguin (Pygoscelis adeliae)           41\nTorgersen  Adelie Penguin (Pygoscelis adeliae)           42\ndtype: int64\n\n\nThe code above lets us break down the numbers by island. Torgerson has only Adelie Penguins, Dream has Chinstraps and Adelies, and Biscoe has Gentoos and Adelies. Based on this, it makes sense that island is the best qualitative feature to use in the model. Knowing which island we are considering instantly simplifes the selection process since we are only choosing between at most two species.\n\n\n\nCulmen length and depth were the two features selected. Let’s see how these vary by species.\n\ntrain[[\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby(\"Species\").mean()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Species\n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      38.710256\n      18.365812\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      48.719643\n      18.442857\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      47.757000\n      15.035000\n    \n  \n\n\n\n\n\ntrain[[\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby(\"Species\").mean().plot.bar(rot=20)\n\n<AxesSubplot: xlabel='Species'>\n\n\n\n\n\nFrom here, we can clearly see that Adelie Penguins have the shortest culmens, while Chinstraps’ culmens are slightly longer than Gentoos’. Gentoos have the smallest culmen depth, while Adelie and Chinstrap are fairly similar in culmen depth. When you consider that, once you know the island, you really only need to be able to distinguish between TWO species (Adelie + one of either Chinstrap or Gentoo), not between all three, it becomes clear that the differences in Culmen Length alone are probably sufficient to make a fairly accurate prediction. Culmen depth is another helpful feature to make the classifier even more accurate.\n\n\nIs a difference in environment enough to change the physical characteristics of a species? Or in this case, of one species, since Adelie penguins are the only ones to appear on more than one island?\n\ntrain[[\"Island\",\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby([\"Island\",\"Species\"]).mean()\n\n\n\n\n\n  \n    \n      \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      Species\n      \n      \n    \n  \n  \n    \n      Biscoe\n      Adelie Penguin (Pygoscelis adeliae)\n      38.674286\n      18.228571\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      47.757000\n      15.035000\n    \n    \n      Dream\n      Adelie Penguin (Pygoscelis adeliae)\n      38.395122\n      18.319512\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      48.719643\n      18.442857\n    \n    \n      Torgersen\n      Adelie Penguin (Pygoscelis adeliae)\n      39.056098\n      18.529268\n    \n  \n\n\n\n\nIt doesn’t look like it, although Adelie Penguins’ Culmen Lengths and Depths are both a tiny bit greater on Torgerson Island than the other two. Maybe the lack of competition from other penguins is responsible for the evolution of a special population of GIANT Adelie penguins on Torgerson. My takeaway is that in 1000 years, Torgerson Island’s penguins will have ENORMOUS culmens.\nBut in all seriousness, no real difference here.\n\n\n\n\nHere, we read in the test data, then score our classifier on the test data filtered to contain only our selected features. We DO NOT fit the classifier to the testing data, because we want it to be scored based on the model trained on the training data. So we first fit it to the TRAINING data, then score it on the testing data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nclf.fit(X_train[top_cols],y_train)\n\nX_test, y_test = prepare_data(test)\nclf.score(X_test[top_cols],y_test)\n\n0.9852941176470589\n\n\n98.5% accuracy. Pretty close to perfect!\n\n\n\nI adapted this technique from the scikit learn documentation page: https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html\nThis shows us the depth of the tree, and how many samples remain to classify at each step. This tree has a depth of 4, which means it needs to make a maximum of 4 decisions to reach a classification for a sample.\n\nfrom sklearn.tree import plot_tree\nfrom matplotlib import pyplot as plt\n\nplot_tree(clf, filled=True)\nplt.title(\"Decision tree trained on 3 Penguin Features: Culmen Length, Depth, and Island\")\nplt.show()\n\n\n\n\n\n\n\n\nX_plot = X_test[top_cols]\nqual_features = [\"Island_Biscoe\",\"Island_Dream\",\"Island_Torgersen\"]\nx0 = X_plot[X_plot.columns[3]]\nx1 = X_plot[X_plot.columns[4]]\n\n\nfrom matplotlib.patches import Patch\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 5))\n\ngrid_x = np.linspace(x0.min() - 1,x0.max() + 1,501)\ngrid_y = np.linspace(x1.min() - 1,x1.max() + 1,501)\nxx, yy = np.meshgrid(grid_x, grid_y)\n\nXX = xx.ravel()\nYY = yy.ravel()\n\nfor i in range(len(qual_features)):\n    XY = pd.DataFrame({\n      X_plot.columns[3] : XX,\n      X_plot.columns[4] : YY\n    })\n\n    for j in qual_features[::-1]:\n        XY.insert(0,j,0)\n\n    XY[qual_features[i]] = 1\n    \n    p = clf.predict(XY.values)\n    p = p.reshape(xx.shape)\n\n    # use contour plot to visualize the predictions\n    axarr[i].contourf(xx, yy, p, cmap='jet',alpha = 0.2,vmin = 0, vmax = 2)\n\n\n    ix = X_plot[qual_features[i]] == 1\n    # plot the data\n    axarr[i].scatter(x0[ix], x1[ix], c = y_test[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n    axarr[i].set_title(label = qual_features[i])\n    axarr[i].set(xlabel = X_plot.columns[3], \n        ylabel  = X_plot.columns[4])\n\n    patches = []\n    for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n\n        plt.tight_layout()\n\n\n\n\nThis is an interesting vizualization. It gives us a glimpse at the blocky nature of the decision tree classifer - rather than separating these data with a single straight line, a Decision Tree Classifier almost breaks the plot up into rectangles.\nThis is because the Decision Tree Classifer works by dividing along one of the two axes at each iteration of the fit process. So it can only divide vertically or horizontally. Over time, it makes smaller and more precise rectangular subsections that classify the data more and more accurately.\n\n\n\nA simple test is to run our classifier with a lower depth and see how well it performs.\n\nclf2 = DecisionTreeClassifier(max_depth = 4)\nclf3 = DecisionTreeClassifier(max_depth = 3)\n\nclf2.fit(X_train[top_cols], y_train)\nscore4 = clf2.score(X_test[top_cols], y_test)\n\nclf3.fit(X_train[top_cols], y_train)\nscore3 = clf3.score(X_test[top_cols], y_test)\n\nscore4,score3\n\n(0.9852941176470589, 0.9852941176470589)\n\n\n\nscore4 == score3\n\nTrue\n\n\nIt looks like the classifier can run well at lower depths. The model still performs well at a depth of 4, and 3. However, even with the depth of five that the classifier automatically selected, the accuracy stays the same and there is no real gap between testing and training data. This means that the model is not overfit to the training data, so while the greater depth may not add anything, it doesn’t detract from the model either.\nIn summary, this was an interesting process. Starting off knowing nothing about the distribution of the species or their physical characteristics, we were able to automate most of the process of building a model to classify them. This speaks to the power of machine learning and how quickly it can break down a complicated process to reach the desired end result.\nObviously not every dataset can be 100% accurately classified as this penguin one could, but a process like this gets us pretty close.\nAnd finally, to honor the three heroes of this blog post, here they are in all their glory:\n  Adelie Penguin  img source\n Chinstrap Penguin img source\n Gentoo Penguin  img source"
  },
  {
    "objectID": "posts/Blog_4/Linear Regression.html",
    "href": "posts/Blog_4/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Source code: https://github.com/wrifro/wrifro.github.io/blob/main/posts/Blog_4/LinearRegression.py"
  },
  {
    "objectID": "posts/Blog_4/Linear Regression.html#conclusion",
    "href": "posts/Blog_4/Linear Regression.html#conclusion",
    "title": "Linear Regression",
    "section": "Conclusion",
    "text": "Conclusion\nThe power of the linear regression model is apparent from these tests. Rather than focusing on labeling datapoints, it is able to come up with a model that predicts what a hypothetical datapoint’s value should be based on the values of the other datapoints. There are slight differences between the different approaches to the model - stochastic gradient descent achieves an optimal result with impressive speed, for example - but at the end of the day all of the different approaches yield more or less the same result."
  },
  {
    "objectID": "posts/Blog_2/GradientDescentBlog.html",
    "href": "posts/Blog_2/GradientDescentBlog.html",
    "title": "Gradient Descent Blog",
    "section": "",
    "text": "Logistic Regression Source Code: click here"
  },
  {
    "objectID": "posts/Blog_2/GradientDescentBlog.html#conclusion",
    "href": "posts/Blog_2/GradientDescentBlog.html#conclusion",
    "title": "Gradient Descent Blog",
    "section": "Conclusion",
    "text": "Conclusion\nBoth the regular gradient descent and stochastic gradient descent are exciting and powerful. They do what the perceptron algorithm cannot, and find the best line if two sets of points are not linearly separable. They run on multiple dimensions, and are quite efficient even if not perfect. Given more time I would have loved to implement stochastic gradient with momentum, but didn’t quite have the time to make it even more efficient. Something for the future."
  },
  {
    "objectID": "posts/example-blog-post/firstPost.html",
    "href": "posts/example-blog-post/firstPost.html",
    "title": "Intro blog post",
    "section": "",
    "text": "And now I’ll try adding it in markdown:\ndef addTwoNumbers(num_a, num_b):\n    sum = num_a + num_b\n    return sum\naddTwoNumbers(3,5)\nLet’s see if this post works…"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference.\n---\ntitle: Intro blog post\nauthor: Wright Frost\ndate: '2023-02-16'\ndescription: 'Just playing around with markdown and quarto'\nformat: html\n---\n\n\n\nI’m going to add some python code:\ndef addTwoNumbers(num_a, num_b):\n    sum = num_a + num_b\n    return sum\naddTwoNumbers(3,5)\n```\n\n\n\nWill this work and make a new post? Not sure. Hopefully yes.\nAnyway….. my name is Wright, I’m a Computer Science and Geography Double major, and this is my attempt at a blog post!"
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html",
    "href": "posts/Blog_1/PerceptronBlog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Perceptron source code: click here"
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html#how-does-the-perceptron-algorithm-work",
    "href": "posts/Blog_1/PerceptronBlog.html#how-does-the-perceptron-algorithm-work",
    "title": "Perceptron Blog",
    "section": "How does the perceptron algorithm work?",
    "text": "How does the perceptron algorithm work?\nThe key part of the perceptron algorithm is the update step. In my code, this is done in both the fit and update methods.\nMathematically, this can be represented as: (on a personal note this is the first thing I have ever done in LaTeX and I’m really proud of it)\n\\(\\hat{w}^{(t+1)} = \\hat{w}^t + \\mathbb{1}(\\tilde{y}_i(\\vec{\\hat{w}^t} \\cdot \\vec{\\tilde{x}_1}) <0){\\tilde{y}_i} {\\tilde{x}_1}\\)\nWithin the fit() method, this is done in the following code:\ny_hat = np.dot(w,X_i)\nw_i = ( (np.multiply(y_hat, y_i) >= 0) * w_i)  + ( (np.multiply(y_hat, y_i) < 0) * (w_i + np.multiply(y_i,X_i)) )\nThe logic is actually fairly simple. Y_hat represents the predicted score for X_i, the current index/value being considered. If y_hat is correctly classified, multiplying it by y_i should yield True, equal to 1 in Python. The other part of the equation checks to see if this product is less than 1 (using 1s and -1s instead of 1s and 0s for our y values makes this possible). If it is, this expression will evaluate to True, which is equal to 1 in Python.\nOnly one of these expressions is true at any given time. When False, these evaluate to zero.\nIf the first expression is True, aka when it is equivalent to 1, it preserves the current weight vector, while the second expression, equal to zero, nullifies its coefficient.\nWhen the second expression is True and therefore equal to 1, it is multiplied by the sum of w_i (the current weight vector) and the product of the feature matrix (X) at index i, and the classification matrix (y) at index i. This sounds complicated, but really this line of code is just either keeping the old weight vector if it classified point i correctly, or adding or subtracting the value it incorrectly classified to ensure that it classifies it correctly in the next iteration.\nThis logic is the same in the update method. The only difference is that update accepts a weight vector as a parameter, running only a single step of the Perceptron algorithm rather than to completion."
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html#if-the-data-are-linearly-separable-then-the-perceptron-algorithm-will-converge.",
    "href": "posts/Blog_1/PerceptronBlog.html#if-the-data-are-linearly-separable-then-the-perceptron-algorithm-will-converge.",
    "title": "Perceptron Blog",
    "section": "If the data are linearly separable, then the perceptron algorithm will converge.",
    "text": "If the data are linearly separable, then the perceptron algorithm will converge.\nLet’s vizualise the process of improvement by showing the progression of the Perceptron algorithm over time.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\np2 = Perceptron()\n\n\n\n\n\nLet’s see how the algorithm updates over time…\nThese three plots only show instances where the weight vector updates (that is to say, when the algorithm misclassifies a point and updates the weight vector to reflect this). This ensures that we actually see how the Perceptron algorithm corrects itself over time. Otherwise, we might just see three cases where it correctly classifies a point and nothing changes. BORING!!!\n\nw_next = np.random.rand(p_features)\n\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\nX_1 = np.append(X, np.ones((X.shape[0], 1)), 1)\nscore_prev = 0\n\nfor ax in axarr.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    w_prev = w_next\n    ax.scatter(X[:,0],X[:,1], c = y)\n    draw_line(w_prev, -10, 10,ax,linestyle = \"dashed\") \n    done = False\n    while done == False:\n        w_next, score, i = p2.update(X,y,w_prev)\n        if (score_prev != score) or score == 1 :\n            done = True\n    score_prev = score\n    draw_line(w_next, -10, 10,ax)\n    ax.scatter(X[i, 0], X[i, 1],color = \"none\", facecolors = \"none\", edgecolors = \"red\")\n    accuracy = (score)\n    ax.set_title(f\"accuracy = {accuracy}\")\n    \nplt.tight_layout()\n\n\n\n\nIt is fairly easy to see that the algorithm updates when it missclassifies a point, gradually improving until it achieves convergence.\n\n\nWhat if the data are not linearly separable?\nLet’s repeat the same experiment as above, only this time, using two clouds of points that are NOT linearly separable.\nWe can achieve this easily with the make_blobs function by making the centers closer to one another.\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.3), (1.7,0.3)])\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nplt.rcParams[\"figure.figsize\"] = (8, 6)\nfig1, axarr1 = plt.subplots(2, 3, sharex = True, sharey = True)\n\nw_next1 = np.random.rand(p_features)\n\nscore_1 = 0\n\nfor ax in axarr1.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    w_prev1 = w_next1\n    ax.scatter(X2[:,0],X2[:,1], c = y2)\n    draw_line(w_prev1, -10, 10,ax,linestyle = \"dashed\") \n    done = False\n    while done == False:\n        score_prev = score1\n        w_next1,score1,i = p2.update(X2,y2,w_prev1)\n        if (score_prev != score1) or score1 == 1 :\n            done = True\n    draw_line(w_next1, -10, 10,ax)\n    ax.scatter(X2[i, 0], X2[i, 1],color = \"none\", facecolors = \"none\", edgecolors = \"red\")\n    accuracy = (score1)\n    ax.set_title(f\"accuracy = {accuracy}\")\n    \nplt.tight_layout()\n\n\n\n\nWe can see that the algorithm still updates and tries to correct itself, but is unable to achieve 100% accuracy. Including more subplots helps to show that this process of guessing but never achieving convergence will continue infinitely.\n\n\nCan Perceptron work in more than 2 dimensions?\nOnly one way to find out…\nLet’s start by generating a 5D feature matrix.\n\nX5d = np.random.rand(100,5)\ny5d = np.random.choice([0, 1], size=100)\n\nWe now have a random 5d feature matrix, and a random binary classification for each point.\nLet’s try to run it with 10,000 max steps…\n\np3 = Perceptron()\n\np3.fit(X5d,y5d, max_steps = 10000)\n\nDid it work?\n\np3.score(X5d,2*y5d-1)\n\n0.6\n\n\nIt works! It makes sense that it wouldn’t achieve convergence on a completely random set of points and classifications.\nIt is worth asking, though, is it theoretically possible to have a linearly separable set of points in 5 (or more) dimensions?\nIt is pretty straightforward to see that this is possible in 3 dimensions - imagine a flat plane separating 2 clouds of points in 3 dimensions.\nIn 5 dimensions, all that this means is that the weight vector multiplied by the feature matrix correctly classifies each point – either one or zero. One does not need to imagine what a 5D physical space might look like to see that this is certainly achievable.\n\n\nPerceptron Runtime\nFinally, let’s consider the question of how long it takes for the update step to run.\nWhat is actually happening? Just multiplication and some logical comparisons, really – we check to see if mathematically the product of our weight vector and the feature matrix evaluates to what we would expect it to be, and if not, we perform some simple matrix multiplication and addition to get our new weight vector. This is dependent on the number of features (p), but not on the number of points (n). Therefore, this operation takes O(p) time. It does not matter how many points there are, because we are only focusing on a single data point at a time, and updating only relative to that point. So the only variable impacting the time complexity of this operation is p – the number of features."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wright’s CS0451 Blog",
    "section": "",
    "text": "Dr. Timnit Gebru comes to Middlebury\n\n\n\n\n\n\nApr 19, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing and testing a linear regression model\n\n\n\n\n\n\nApr 9, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining a model to classify penguins\n\n\n\n\n\n\nMar 25, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing and Exploring the Perceptron Algorithm\n\n\n\n\n\n\nMar 10, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing and Exploring the Perceptron Algorithm\n\n\n\n\n\n\nFeb 28, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust playing around with markdown and quarto\n\n\n\n\n\n\nFeb 16, 2023\n\n\nWright Frost\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]