[
  {
    "objectID": "posts/example-blog-post/firstPost.html",
    "href": "posts/example-blog-post/firstPost.html",
    "title": "Intro blog post",
    "section": "",
    "text": "And now I’ll try adding it in markdown:\ndef addTwoNumbers(num_a, num_b):\n    sum = num_a + num_b\n    return sum\naddTwoNumbers(3,5)\nLet’s see if this post works…"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference.\n---\ntitle: Intro blog post\nauthor: Wright Frost\ndate: '2023-02-16'\ndescription: 'Just playing around with markdown and quarto'\nformat: html\n---\n\n\n\nI’m going to add some python code:\ndef addTwoNumbers(num_a, num_b):\n    sum = num_a + num_b\n    return sum\naddTwoNumbers(3,5)\n```\n\n\n\nWill this work and make a new post? Not sure. Hopefully yes.\nAnyway….. my name is Wright, I’m a Computer Science and Geography Double major, and this is my attempt at a blog post!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing and Exploring the Perceptron Algorithm\n\n\n\n\n\n\nFeb 27, 2023\n\n\nWright Frost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust playing around with markdown and quarto\n\n\n\n\n\n\nFeb 16, 2023\n\n\nWright Frost\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Blog_1/Untitled1.html",
    "href": "posts/Blog_1/Untitled1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "def gradient(w1,w2):\n    (w2 * np.cos(w1 * w2),w1 * np.cos(w1 * w2))"
  },
  {
    "objectID": "posts/Blog_1/PerceptronBlog.html",
    "href": "posts/Blog_1/PerceptronBlog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "from perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\np = Perceptron()\np.fit(X, y,1000)\n\nIf we print the last ten values in the Perceptron history array – that is to say, how accurate the last ten values were, we see that 99% of points were correctly classified for 9 of the last ten iterations, before the algorithm achieves 100% accuracy. 100% accuracy is achieved when the algorithm misclassifies a point and makes one final correction. It makes sense that there would be many instances of a 99% accurate line before the final update, since in order to update the algorithm needs to incorrectly classify a point. If 99% of points are correctly classfied, it will take a long time to randomly select a point that is misclassified and forces the algorithm to update.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nWe can further highlight this by showing the change in accuracy over time.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max,pos,**kwargs): # \"pos\" = position, i.e. where to plot (plt, ax)\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  pos.plot(x, y, color = \"black\", linestyle = \"dashed\" if kwargs else \"solid\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2,plt)\n#fig = draw_line(p.w[-2], -2, 2,plt,linestyle = \"dashed\")\n\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np.score(X, (2*y-1))\n\n1.0\n\n\n\nIf the data are linearly separable, then the perceptron algorithm will converge.\nLet’s vizualise this showing the progression of the Perceptron algorithm over time.\nStep one is genarating two clusters of points, nearly exactly the same as those used above.\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.3), (1.7,1.6)])\n\nfig = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\np2 = Perceptron()\n\n\n\n\n\nLet’s see how the algorithm updates over time if the data are linearly separable.\nThese three plots only show instances where the weight vector updates (that is to say, when the algorithm misclassifies a point and updates the weight vector to reflect this).\n\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\n\nw_next = np.random.rand(p_features)\n\nfor ax in axarr.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    w_prev = w_next\n    ax.scatter(X1[:,0],X1[:,1], c = y1)\n    draw_line(w_prev, -10, 10,ax,linestyle = \"dashed\") \n    done = False\n    while done == False:\n        score_prev = score\n        w_next,score,i = p2.update(X1,y1,w_prev)\n        if (score_prev != score) or score == 1 :\n            done = True\n    draw_line(w_next, -10, 10,ax)\n    ax.scatter(X1[i, 0], X1[i, 1],color = \"none\", facecolors = \"none\", edgecolors = \"red\")\n    accuracy = (score)\n    ax.set_title(f\"accuracy = {accuracy}\")\n    \nplt.tight_layout()\n\n\n\n\n\n\n\nWhat if the data are not linearly separable?\nLet’s repeat the same experiment as above, only this time, using two clouds of points that are NOT linearly separable.\nWe can achieve this easily with the make_blobs function by making the centers closer to one another.\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.3), (1.7,0.3)])\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nfig1, axarr1 = plt.subplots(2, 3, sharex = True, sharey = True)\n\nw_next1 = np.random.rand(p_features)\n\nfor ax in axarr1.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    w_prev1 = w_next1\n    ax.scatter(X2[:,0],X2[:,1], c = y2)\n    draw_line(w_prev1, -10, 10,ax,linestyle = \"dashed\") \n    done = False\n    while done == False:\n        score_prev = score1\n        w_next1,score1,i = p2.update(X2,y2,w_prev1)\n        if (score_prev != score1) or score1 == 1 :\n            done = True\n    draw_line(w_next1, -10, 10,ax)\n    ax.scatter(X2[i, 0], X2[i, 1],color = \"none\", facecolors = \"none\", edgecolors = \"red\")\n    accuracy = (score1)\n    ax.set_title(f\"accuracy = {accuracy}\")\n    \nplt.tight_layout()\n\n\n\n\nWe can see that the algorithm still updates and tries to correct itself, but is unable to achieve total accuracy. Including more subplots shows that this process of guessing but never achieving convergence will continue infinitely."
  }
]